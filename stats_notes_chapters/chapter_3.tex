%\section{Discrete Random Variables}
%\begin{mydef}{Joint Frequency Function}{def_pmf}
%If $X$ and $Y$ are discrete random variables defined on the same sample space and they take on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$, respectively. The \textbf{joint frequency function/joint probability mass function} $p(x,y)$ is defined as
%\[p(x_i, y_i) = P(X = x_i, Y = y_i).\]
%\end{mydef}
%
%\begin{example}{}{}
%	A fair coin is tossed three times. Let $X$ denote the number of heads on the first toss and $Y$ the total number of heads. 
%	\begin{enumerate}
%		\item $p(x,y)$
%		\item Find the frequency function of $Y$, $p_Y$, from the joint frequency function.
%	\end{enumerate}
%\end{example}
%
%\begin{solution}
%\begin{enumerate}
%	\item Based on the sample space
%	\[\Omega = \{hhh, hht, hth, htt, thh, tht, tth, ttt\}\]
%	the joint frequency function of $X$ and $Y$ is given by the following table
%	\begin{center}
%		\begin{tabular}{c|c c c c } 
%			& $y$ & & &  \\ \hline
%		   $x$ & 0 & 1 & 2 & 3  \\ \hline
%		   0 & 1/8 & 2/8 & 1/8 & 0 \\
%		   1 & 0 & 1/8 & 2/8 & 1/8
%		\end{tabular}
%	\end{center}
%	
%	\item The frequency function of $Y$ is given by the following table
%	\begin{center}
%		\begin{tabular}{c| c c c c}
%			$y$ & 0 & 1 & 2 & 3 \\ \hline
%			$p_Y$ & 1/8 & 3/8 & 3/8 & 1/8 
%		\end{tabular}
%	\end{center}
%	To derive this table, for example, to derive $p_Y(0)$, we compute:
%	\begin{eqnarray*}
%		p_Y(0) &=& P(Y = 0)\\
%		&=& P(Y = 0, X = 0) + P(Y = 0, X = 1) \\
%		&=& \frac{1}{8} + 0 \\
%		&=& \frac{1}{8}
%	\end{eqnarray*}
%\end{enumerate}
%\end{solution}
%
%\begin{remark}
%If $p(x,y)$ is the joint frequency function of $X$ and $Y$, then \textbf{marginal frequency distribution} of $X$ is given by
%\[p_X(x) = \sum_{i}p(x, y_i)\]
%and the \textbf{marginal frequency distribution} of $Y$ is given by
%\[p_Y(y) = \sum_{i}p(x_i, y).\]
%\end{remark}
%
%\begin{remark}
%More generally, if $X_1, \ldots, X_{m}$ are discrete random variables defined on the same sample space, their joint frequency function is
%\[p(x_1, \ldots, x_m) = P(X_1 = x_1, \ldots, X_m = x_m).\]
%The marginal frequency function of $X_1$, for example, is
%\[p_{X_1}(x_1) = \sum_{x_2\ldots x_m}p(x_1, x_2, \ldots, x_m).\]
%The two-dimensional marginal frequency function of $X_1$ and $X_2$, for example, is
%\[p_{X_{1}X_{2}}(x_1, x_2) = \sum_{x_3\ldots x_m}p(x_1, x_2, \ldots, x_m).\]
%\end{remark}
%
%\begin{example}{Multinomial Distribution}{}
%Suppose each of $n$ independent trials can result in one of $r$ types of outcomes and that on each trial the probability of the $r$ outcomes are $p_1, p_2, \ldots, p_r$. Let $N_i, 1\leq i\leq r$ be the total number of outcomes of type $i$ in the $n$ trials. 
%\begin{enumerate}
%	\item Find the joint frequency function of $N_1, \ldots, N_r$.
%	\item Find the marginal distribution of any particular $N_i$.
%\end{enumerate}
%\end{example}
%
%\begin{solution}
%	\begin{enumerate}
%		\item The joint frequency function of $N_1, \ldots, N_r$ is
%		\[p(n_1, \ldots, n_r) = {n\choose n_1\ldots n_r}p_1^{n_1}\ldots p_r^{n_r}.\]
%		The reason is that any sequence of trials that gives rise to $N_1 = n_1, \ldots, N_r = n_r$ occurs with probability $p_1^{n_1}\ldots p_r^{n_r}$ and there are
%		$\frac{n!}{n_{1}!\ldots n_{r}!}$ such sequences.
%		\item The variable $N_i$ can be interpreted as the number of successes in $n$ trials, each of which has probability $p_i$ of success and $1 - p_i$ of failure. Therefore, $N_i$ is a binomial random variable and 
%		\[p_{N_{i}}(n_i) = {n\choose n_i}p_{i}^{n_i}(1-p_i)^{n-n_i}.\]
%	\end{enumerate}
%\end{solution}
%
%\section{Continuous Random Variables}
%\begin{mydef}{Joint Density Function}{}
%	If $X$ and $Y$ are continuous random variables, then their \textbf{joint density function} $f(x,y)$ is a piecewise continuous function with the following properties:
%	\begin{enumerate}
%		\item $f(x,y) \geq 0$.
%		\item $\displaystyle \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y) = 1$.
%		\item $\displaystyle P((X, Y)\in R) =  \iint_{R}f(x,y)dA$ for any ``reasonable'' two-dimensional set $R$.
%	\end{enumerate}
%\end{mydef}
%\begin{remark}
%	If $\displaystyle F(x,y) = P(X\leq x, Y\leq y)$ is the joint cdf of $X$ and $Y$, then
%	\[F(x,y) = \int_{-\infty}^{x}\int_{-\infty}^{y}f(u,v)dvdu\]
%	and
%	\[f(x,y) = \frac{\partial^{2}}{\partial x\partial y}F(x,y)\]
%	whenever the derivative is defined.
%\end{remark}
%
%\begin{remark}
%The \textbf{marginal cdf} of $X$ is 
%\begin{eqnarray*}
%	F_{X}(x) &=& P(X\leq x)\\
%	&=& \lim_{y\to\infty}F(x,y)\\
%	&=& \int_{-\infty}^{x}\int_{-\infty}^{\infty}f(u,y)dydu.
%\end{eqnarray*}
%The \textbf{marginal density} of $X$ is
%\[f_X(x) = F_{X}'(x) = \int_{-\infty}^{\infty}f(x,y)dy.\]
%\end{remark}
%
%\begin{example}{}{}
%Given the bivariate density function
%\[f(x,y) = \frac{12}{7}(x^2 + xy), 0\leq x\leq 1, 0\leq y\leq 1.\]
%\begin{enumerate}
%	\item Find $P(X > Y)$.
%	\item Find $f_X(x)$ and $f_Y(y)$.
%\end{enumerate}
%\end{example}
%
%\begin{solution}
%	\begin{enumerate}
%		\item \begin{eqnarray*}
%			P(X > Y) &=& \frac{12}{7}\int_{0}^{1}\int_{0}^{x}(x^2 + xy)dydx \\
%			&=& \frac{9}{14}
%		\end{eqnarray*}
%		\item \[
%			f_X(x) = \frac{12}{7}\int_{0}^{1}(x^2 + xy)dy = \frac{12}{7}(x^2 + \frac{x}{2})
%		\]
%		\[
%			f_Y(y) = \frac{12}{7}\int_{0}^{1}(x^2 + xy)dx = \frac{12}{7}(\frac{1}{3} + \frac{y}{2}).
%		\]
%	\end{enumerate}
%\end{solution}
%
%\begin{myprop}{Farlie-Morgenstern Family}{}
%	If $F(x)$ and $G(y)$ are one-dimensional cdfs, then for any $\alpha$ with $|\alpha| \leq 1$, 
%	\[H(x,y) = F(x)G(y)[1 + \alpha[1-F(x)][1-G(y)]]\]
%	is a bivariate cumulative distribution function with marginal distributions $F(x)$ and $G(y)$.
%\end{myprop}
%
%\begin{example}{}{}
%Consider the following joint density
%\[ f(x,y) = \begin{cases} 
%	\lambda^{2}e^{-\lambda y}, & 0\leq x\leq y, \lambda > 0 \\
%	0, & \text{elsewhere} 
%\end{cases}
%\]
%Find the marginal densities.
%\end{example}
%
%
%%\begin{mydef}{Joint CDF}{def_joint_cdf}
%%Some Definition here
%%\end{mydef}
%%I can refer to definition ~\ref{defn:def_joint_cdf}
%%
%%\begin{mythm}{Independent}{thm_independent}
%%Some Theorem here
%%\end{mythm}
%%
%%I can refer to Theorem ~\ref{thm:thm_independent}
%%
%%\begin{myprop}{Some proposition}{some_prop}
%%Here is a proposition
%%\end{myprop}
%%
%%I can refer to proposition ~\ref{prop:some_prop}
%%
%%\begin{exercise}{}{}
%%Prove that 
%%\[x^n + y^n = z^n\]
%%\end{exercise}
%%
%%\begin{proof}
%%	Some proof here.
%%\end{proof}
%
%%\begin{remark}
%%Here is a remark
%%\end{remark}
%%\section{Continuous Random Variables}
%
%%\section{Indepenent Random Variables}



\section{Bivariate and Multivariate Probability Distributions}

\begin{mydef}{Joint Probability Function for Discrete RVs}{def_jpf_discrete}
	The \emph{joint (or bivariate) probability function} for discrete random variables $Y_1$ and $Y_2$ is given by
	\[p(y_1, y_2) = P(Y_1 = y_1, Y_2 = y_2), -\infty<y_1, y_2<\infty.\]
\end{mydef}

\begin{remark}
	The joint probability function $p$ of two discrete random variables $Y_1$ and $Y_2$ satisfies:
	\begin{enumerate}
		\item $p(y_1, y_2)\geq 0$ for all $y_1, y_2$.
		\item $\displaystyle \sum_{y_1, y_2}p(y_1, y_2) = 1$ 
	\end{enumerate}
\end{remark}

\begin{mydef}{Joint Distribution Function}{def_jdf}
The \emph{joint (bivariate) ditribution function} for any random variables $Y_1$ and $Y_2$ is given by
\[F(y_1, y_2) = P(Y_1 \leq y_1, Y_2\leq y_2), -\infty<y_1, y_2 < \infty.\]
\end{mydef}

\begin{mydef}{Jointly Continuous RVs}{def_jpdf}
	Let $Y_1$ and $Y_2$ be continuous RVs with joint distribution function $F(y_1, y_2)$. If there exists a nonnegative function $f(y_1, y_2)$ such that
	\[F(y_1, y_2) = \int_{-\infty}^{y_1}\int_{-\infty}^{y_2}f(t_1, t_2) dt_{2} dt_{1}\]
	for all $-\infty < y_1, y_2 < \infty$ then $Y_1$ and $Y_2$ are said to be \emph{jointly continuous RVs}. The function $f(y_1, y_2)$ is called the \emph{joint probability density function}.
\end{mydef}

\begin{remark}
	If $f(y_1, y_2)$ is a joint density function for jointly continuous random variables $Y_1$ and $Y_2$, then
	\begin{enumerate}
		\item $f(y_1, y_2)\geq 0$ for all $y_1, y_2$.
		\item $\displaystyle \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(y_1, y_2)dy_{1}dy_{2} = 1$.
	\end{enumerate}
\end{remark}
\begin{remark}
	$\displaystyle P((X, Y)\in R) =  \iint_{R}f(x,y)dA$ for any ``reasonable'' set $R$ in the $(y_1, y_2)$ plane. 
\end{remark}

{\huge \textbf{Exercises}}
\vspace{10pt}
\begin{exercise}{}{}
Contracts for two construction jobs are randomly assigned to one or more of three firms A, B, C. Let $Y_1 = \#\text{contracts assigned to firm }A$ and $Y_2 = \#\text{contracts assigned to firm }B$.
\begin{enumerate}
	\item Find the joint probability function for $Y_1$ and $Y_2$.
	\item Find $F(1,0)$.
\end{enumerate}
\end{exercise}

\begin{solution}
	\begin{enumerate}
		\item Let $(\alpha, \beta)$ where $\alpha, \beta\in \{A, B, C\}$ denote the firms that job 1 and job 2 are assigned to, respectively. The sample space is
		\[\Omega = \{(A,A), (A,B), (A,C), (B,A), (B,B), (B,C), (C, A), (C,B), (C,C)\}.\]
		Each outcome has probability 1/9. The joint probability function for $Y_1$ and $Y_2$ is given by the following table
		\begin{center}
			\begin{tabular}{c | c c c}
				& & $y_1$ &  \\ \cline{2-4}
			  $y_2$ & 0 & 1 & 2 \\ \hline
			  0 & 1/9 & 2/9 & 1/9 \\
			  1 & 2/9 & 2/9 & 0\\
			  2 & 1/9 & 0 & 0
			\end{tabular}
		\end{center}
		\item We have
		\begin{eqnarray*}
			F(1,0) &=& P(Y_{1}\leq 1, Y_{2}\leq 0)\\
			&=& P(Y_1 = 0, Y_2 = 0) + P(Y_1 = 1, Y_2 = 0)\\
			&=& p(0,0) + p(1,0)\\
			&=& \frac{1}{9} + \frac{2}{9}\\
			&=& \frac{1}{3} 
		\end{eqnarray*}
	\end{enumerate}
\end{solution}

\begin{exercise}{}{}
Three balanced coins are tossed independently. Let $Y_1 = \#\text{heads}$ and $Y_2 = \text{ amount of money won}$. The amount of money won is determined as follows: if the first head occurs on toss $i$, you win $\$i$ for $i=1, 2\text{ or }3$. If no heads appear, you lose $\$1$.
	\begin{enumerate}
		\item Find $p(y_1, y_2)$.
		\item Find the probability that fewer than three heads will occur and you will win $\$1$ or less, i.e., find $F(2,1)$.
	\end{enumerate}
\end{exercise}
\begin{solution}
	\begin{enumerate}
		\item The function $p(y_1, y_2)$ is given by
		\begin{center}
			\begin{tabular}{c | c c c c}
				& & $y_1$ & & \\ \cline{2-5}
				$y_2$& 0 & 1 & 2 & 3 \\ \hline 
				$-1$ & 1/8 & 0 & 0 & 0 \\
				1 & 0 & 1/8 & 2/8 & 1/8 \\
				2 & 0 & 1/8 & 1/8 & 0 \\
				3 & 0 & 1/8 & 0 & 0
			\end{tabular}
		\end{center}
		\item We have
		\begin{eqnarray*}
			F(2,1) &=& p(0,-1) + p(1,1) + p(2,1) \\
			&=& \frac{1}{8} + \frac{1}{8} + \frac{2}{8}\\
			&=& \frac{1}{2} 
		\end{eqnarray*}
	\end{enumerate}
\end{solution}

\begin{exercise}{}{}
	Of nine executives in a business firm, 4 are married, 3 have never married and 2 are divorced. 3 of the executives are to be randomly selected for promotion. Let $Y_1$ be the number of married executives and $Y_2$ be the number of never-married executives among the 3 selected. Find the joint probability function of $Y_1$ and $Y_2$.
\end{exercise}

\begin{solution}
	The sample space, the possible values of $Y_1$ and $Y_2$ and the associated probabilities are given by
	\begin{center}
		\begin{tabular}{|c|c| c| c| c| c| c| c| c|c|}
			\hline
			 outcome & 3m & 2m1n & 2m1d & 3n & 2n1m & 2n1d & 2d1m & 2d1n & 1m1n1d \\ \hline
			 $(y_1, y_2)$ & $(3,0)$ & $(2,1)$ & $(2,0)$ & $(0,3)$ & $(1,2)$ & $(0,2)$ & $(1,0)$ & $(0,1)$ & $(1,1)$ \\ \hline
			 probability & $\frac{{4\choose 3}}{{9\choose 3}}$ & $\frac{{4\choose 2}{3\choose 1}}{{9\choose 3}}$ & $\frac{{4\choose 2}{2\choose 1}}{{9\choose 3}}$ & $\frac{{3\choose 3}}{{9\choose 3}}$ & $\frac{{4\choose 1}{3\choose 2}}{{9\choose 3}}$ & $\frac{{3\choose 2}{2\choose 1}}{{9\choose 3}}$ & $\frac{{4\choose 1}}{{9\choose 3}}$ & $\frac{{3\choose 1}}{{9\choose 3}}$ &  $\frac{{4\choose 1}{3\choose 1}{2\choose 1}}{{9\choose 3}}$ \\ \hline
		\end{tabular}
	\end{center}
	The joint probability function $p(y_1, y_2)$ is given by
	\begin{center}
		\begin{tabular}{c| c c c c }
			& & $y_1$ & & \\ \cline{2-5}
			$y_2$ & 0 & 1 & 2 & 3 \\ \hline
			0 & 0 & 1/21 & 1/7 & 1/21\\
			1 & 1/28 & 2/7 & 3/14 & 0\\
			2 & 1/14 & 1/7 & 0 & 0\\
			3 & 1/84 & 0 & 0 &  0\\
		\end{tabular}
	\end{center}
	More concisely, one can write down the formula for $p(y_1,y_2)$ as
	\[p(y_1, y_2) = \frac{{4\choose y_1}{3\choose y_2}{2\choose 3-y_1-y_2}}{{9\choose 3}}.\]
\end{solution}